# transformers-and-vit-from-scratch
Implementation of Transformer and Vision Transformer architectures from scratch using PyTorch.

## About

This project implements the core components of transformer architectures to gain a deeper understanding of how modern deep learning models work. All components are built from first principles without relying on pre-built transformer libraries.

## Architecture Components

**Transformer:**
1. Tokenization
2. Embedding Layer
3. Positional Encoding
4. Scaled Dot-Product Attention
5. Multi-Head Attention
6. Feed-Forward Network
7. Layer Normalization
8. Encoder Block

**Vision Transformer (ViT):**
1. Patch Embedding
2. Position Embeddings
3. Class Token
4. ViT Encoder Block
5. Classification Head
6. Complete ViT

## Implementation Status

- [x] Tokenization
- [x] Embedding Layer
- [x] Positional Encoding
- [ ] Scaled Dot-Product Attention
- [ ] Multi-Head Attention
- [ ] Feed-Forward Network
- [ ] Layer Normalization
- [ ] Encoder Block
- [ ] Vision Transformer (ViT)

## References

- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - Original Transformer paper
- [An Image is Worth 16x16 Words](https://arxiv.org/abs/2010.11929) - Vision Transformer paper

## License

MIT
